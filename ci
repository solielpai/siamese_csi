import os
import glob
import time
import numpy as np
import tensorflow as tf
from skimage import io, transform
 
# os.environ["TF_CPP_MIN_LOG_LEVEL"] = '1'
# 这是默认的显示等级，显示所有信息
# os.environ["TF_CPP_MIN_LOG_LEVEL"] = '2'
# 只显示 warning 和 Error
os.environ["TF_CPP_MIN_LOG_LEVEL"] = '3'
# 只显示 Error
 
 
# 读取图片
def read_img(path, w, h):
#    for x in os.listdir(path):
#        print(path+x)
#        
#        if os.path.isdir(path +'\\'+ x):
#            
#            cate=[path+'\\'+ x]
#            print(cate)
    cate = [path+'\\'+ x for x in os.listdir(path) if os.path.isdir(path+'\\'+ x)]
    #print(cate)
 
    imgs = []
    labels = []
 
    #print('Start read the image ...')
 
    for index, folder in enumerate(cate):
        #print(index, folder)
        for im in glob.glob(folder + '/*.jpg'):
            #print('Reading The Image: %s' % im)
            img = io.imread(im)
            img = transform.resize(img, (w, h,3))
            imgs.append(img)
            labels.append(index)
 
    print('Finished ...')
 
    return np.asarray(imgs, np.float32), np.asarray(labels, np.float32)
data, label = read_img(path='E:\CSI3', w=30, h=30)
#print(label) 

# 打乱顺序
def messUpOrder(data, label):
    num_example = data.shape[0]
    
    arr = np.arange(num_example)
    np.random.shuffle(arr)
    data = data[arr]
    label = label[arr]
    label=tf.one_hot(label,277, 1, 0)
    sess=tf.Session()
    sess.run(tf.global_variables_initializer())
    label=label.eval(session=sess)
    return data, label
 
# 
## 将所有数据分为训练集和验证集
def segmentation(data, label, ratio=0.8):
    num_example = data.shape[0]
    s = np.int(num_example * ratio)
    x_train = data[:s]
    y_train = label[:s] 
    x_val = data[s:]
    y_val = label[s:]
 
    return x_train, y_train, x_val, y_val
def we(shape):
    ini=tf.truncated_normal(shape,stddev=0.1)
    return tf.Variable(ini)
def bi(shape):
    ini=tf.constant(0.1,shape=shape)
    return tf.Variable(ini)
def conv2d(x,w):
    return tf.nn.conv2d(x,w,strides=[1,1,1,1],padding='SAME')
def max_pool(x):
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')
# 
# 
## 构建网络
def buildCNN(w, h, c):
    # 占位符
    x = tf.placeholder(tf.float32, shape=[None, w, h, c], name='x')
    y_ = tf.placeholder(tf.int32, shape=[None,277], name='y_')
 
    
    w_1=we([3,3,3,32])
    b_1=bi([32])
    x_image=tf.reshape(x,[-1,30,30,3])
    h_conv1=tf.nn.relu(conv2d(x_image,w_1)+b_1)
    h_pool1=max_pool(h_conv1)
#第二个卷积层
    w_2=we([3,3,32,64])
    b_2=bi([64])
    h_conv2=tf.nn.relu(conv2d(h_pool1,w_2)+b_2)
    h_pool2=max_pool(h_conv2)
#全连接层
    w_c1=we([8*8*64,1024])
    b_c1=bi([1024])
    x_1=tf.reshape(h_pool2,[-1,8*8*64])
    h_f1=tf.nn.relu(tf.matmul(x_1,w_c1)+b_c1)

    #keep_prob=tf.placeholder('float')
    h_d=tf.nn.dropout(h_f1,0.6)

#性能评估
    w_c2=we([1024,277])
    b_c2=bi([277])
    logits=tf.nn.softmax(tf.matmul(h_d,w_c2)+b_c2)
    
    return logits, x, y_
 
 
# 返回损失函数的值，准确值等参数
def accCNN(logits, y_):
    
    
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))
    train_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss)
    correct_prediction = tf.equal(tf.argmax(logits, 1),tf.argmax( y_,1))
    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print(acc)
    return loss, train_op, correct_prediction, acc
 
 
# 定义一个函数，按批次取数据
#def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):
##    assert tf.shape(inputs) == len(targets)
#    if shuffle:
#        indices = np.arange(len(inputs))
#        print(len(inputs),inputs.shape,targets.shape,'输入len')
#        np.random.shuffle(indices)
#    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):
#        print(start_idx,'start_idx')
#        if shuffle:
#            excerpt = indices[start_idx:start_idx + batch_size]
#            print(excerpt.shape,'excerpt')
#        else:
#            excerpt = slice(start_idx, start_idx + batch_size)
#          
#       # print(excerpt.shape,'shape',excerpt,'excerpt')
#        print(inputs[excerpt].shape,targets[excerpt].shape,'输出shape')
#        return( inputs[excerpt], targets[excerpt])
#        print(inputs.shape,'inputs')
def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):
#    assert tf.shape(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        #print(len(inputs),inputs.shape,targets.shape,'输入len')
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):
        #print(start_idx,'start_idx')
        if shuffle:
            excerpt = indices[start_idx:start_idx + batch_size]
            #print(excerpt.shape,'excerpt')
        else:
            excerpt = slice(start_idx, start_idx + batch_size)
          
       # print(excerpt.shape,'shape',excerpt,'excerpt')
        #print(inputs[excerpt].shape,targets[excerpt].shape,start_idx,'输出shape')
        yield( inputs[excerpt], targets[excerpt]) 
 
def runable(x_train, y_train, train_op, loss, acc, x, y_, x_val, y_val):
    # 训练和测试数据，可将n_epoch设置更大一些
    n_epoch = 500
    batch_size = 16
    sess = tf.InteractiveSession()
    sess.run(tf.global_variables_initializer())
    #print('here is ok2') 
    for epoch in range(n_epoch):
        # training
        #print('here is ok1')
        train_loss, train_acc, n_batch = 0, 0, 0
        for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):
            #print(x_train_a.shape, y_train_a.shape,'here is ok')
             _,err, ac = sess.run([train_op, loss, acc], feed_dict={x: x_train_a, y_: y_train_a})
             train_loss += err
             train_acc += ac
             n_batch += 1
            #print(n_batch) 
#            print(y_train_a)
#            print(logits_,'y',y_train_a)
        print("train loss: %f" % (train_loss / n_batch))
        print("train acc: %f" % (train_acc / n_batch))
 
        # validation
        val_loss, val_acc, n_batch = 0, 0, 0
        for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):
            err, ac = sess.run([loss, acc], feed_dict={x: x_val_a, y_: y_val_a})
            val_loss += err
            val_acc += ac
            n_batch += 1
        print("validation loss: %f" % (val_loss / n_batch))
        print("validation acc: %f" % (val_acc / n_batch))
        print('*' * 50)
 
    sess.close()
 
 
if __name__ == '__main__':
    imgpath = '../dataset/classify/'
 
    w = 30
    h = 30
    c = 3
 
    ratio = 0.8  # 选取训练集的比例
 #read_img('E:\CSI1',30,30) 
    data, label = read_img(path='E:\CSI3', w=30, h=30)
    #print(label)
    
    data, label = messUpOrder(data=data, label=label)
    sess=tf.Session()
#sess.run(train_labes, feed_dict={x: 100, y: 200})
#    print(sess.run(label[0:16]),'label')
#    print(label[0:16],'label')
    x_train, y_train, x_val, y_val = segmentation(data=data, label=label, ratio=ratio)
    #print(x_train.shape,'x',y_train.shape,'qq')
    logits, x, y_ = buildCNN(w=w, h=h, c=c)
    print(logits,y_,'输入与预测')
    loss, train_op, correct_prediction, acc = accCNN(logits=logits, y_=y_)
    #print(loss,'loss')
    runable(x_train=x_train, y_train=y_train, train_op=train_op, loss=loss,
            acc=acc, x=x, y_=y_, x_val=x_val, y_val=y_val)
