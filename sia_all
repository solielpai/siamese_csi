import os
import glob
import time
import numpy as np
import tensorflow as tf
from skimage import io, transform
import warnings
import inference
import pandas as pd
warnings.filterwarnings('ignore')
os.environ["TF_CPP_MIN_LOG_LEVEL"] = '3'
# 只显示 Error

f= open(r'C:\Users\soliel\Desktop\r.txt','a+')#
nu=4
nl=6
lr=0.01
pat=r'E:\dda'
pat1=r'E:\ddb'
w = 30
h = 30
c = 3
s=[]
n_e=201
b_s=32
# 读取图片
def read_img(path, w, h, num,nl,xr):


    cate = [path+'\\'+ x for x in os.listdir(path) if os.path.isdir(path+'\\'+ x)]
    i=0
    imm=[]
    iii=[]
    imgs = []
    labels = []
    labels1 = []
    j=0
    vi=[]
    vl=[]
    xxx = [[[0 for i in range(2700)]] for l in range(100)]
    for index, folder in enumerate(cate):
        j = 0

        for im in glob.glob(folder + '/*.jpg'):


            #print(folder)

            b=folder.lstrip(xr)#
            #print(b,'dsffsd')
            b=int(b)
            img = io.imread(im)
            img = transform.resize(img, [w,h,3])
            img=img.reshape([2700])
            if j < num:
                j += 1
                imm.append(img)
                imgs.append(img)
                labels.append(b)
                if j % num == 0:
                    a = np.sum(imm, axis=0)

                    c = a / num
                        # j += 1
                    iii.append(imm)
                    xxx[b - 1] = imm
                    imm = []
                    labels1.append(b)
            if num<=j<nl:
            #else:
                j += 1
                vi.append(img)
                    #vl.append(img)
                vl.append(b)

    print('Finished ...')


    
    return imgs, labels,iii, labels1,vi,vl,xxx
data, label ,va,l,vi,vl,x1= read_img(path=pat, w=30, h=30,num=nu,nl=nl,xr=r'E:\dda/')
d1, l1 ,v1,l2,vi1,vl1,x2= read_img(path=pat1, w=30, h=30,num=nu,nl=nl,xr=r'E:\ddb/')
data=data+d1 #两次采集的CSI图片生成的总的list ，用于离线阶段
label=label+l1#两次采集的CSI图片对应的标签，用于离线阶段
#print(x1)
vi=vi+vi1 #在线阶段的CSI图片
vl=vl+vl1#在线阶段的标签
def cha(a,b,va,v1,l,l1,e,f,x1,x2):

    for i in range(100):
        #print(len(va))
       # print(i,x1[i],'qweqewrewtretetr')
        imm=x1[i]+x2[i]
        print(len(imm))
        x1[i]=np.sum(imm, axis=0)/len(imm)
       # print(x1[i],'safsff')
    l=[xx+1 for xx in range(100)]
    return np.asarray(a, np.float32), np.asarray(b, np.float32),np.asarray(x1, np.float32), np.asarray(l, np.float32),np.asarray(e, np.float32), np.asarray(f, np.float32)
data, label ,va,l,vi,vl= cha(data,label,va,v1,l,l2,vi,vl,x1,x2)
print(data.shape,label.shape,va.shape,l.shape)
#print(vl)
def messUpOrder(data, label):
    num_example = data.shape[0]
    
    arr = np.arange(num_example)
    np.random.shuffle(arr)
  #  print(arr)
    data = data[arr]
    label = label[arr]
#    label=tf.one_hot(label,277, 1, 0) 未进行独热编码
#    sess=tf.Session()
#    sess.run(tf.global_variables_initializer())
#    label=label.eval(session=sess)
    return data, label
x_train, y_train=messUpOrder(data, label)  #实际训练时的训练集和标签
x1,y1=messUpOrder(va, l) #指纹数据
x_val, y_val=vi,vl  #在线阶段的测试集和标签
print(x_val.shape)
ss=list(0 for x in range(100))
## 将所有数据分为训练集和验证集
def segmentation(data, label, ratio):#暂时不需要
    num_example = data.shape[0]
    s = np.int(num_example * ratio)
    x_train = data[:s]
    y_train = label[:s] 
    x_val = data[s:]
    y_val = label[s:]

    return x_train, y_train, x_val, y_val
#x_train, y_train, x_val, y_val=segmentation(data, label, ratio=0.8)



#import visualize

# prepare data and tf.session
#mnist = input_data.read_data_sets('MNIST_data', one_hot=False)
sess = tf.InteractiveSession()
tf.reset_default_graph() 
# setup siamese network   
siames = inference.siamese()
train_step = tf.train.GradientDescentOptimizer(lr).minimize(siames.loss)
saver = tf.train.Saver()
#tf.initialize_all_variables().run()
tf.Session().run(tf.global_variables_initializer())
def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):
#    assert tf.shape(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        #print(len(inputs),inputs.shape,targets.shape,'输入len')
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):
        #print(start_idx,'start_idx')
        if shuffle:
            excerpt = indices[start_idx:start_idx + batch_size]
            #print(excerpt.shape,'excerpt')
        else:
            excerpt = slice(start_idx, start_idx + batch_size)  
          
       # print(excerpt.shape,'shape',excerpt,'excerpt')
        #print(inputs[excerpt].shape,targets[excerpt].shape,start_idx,'输出shape')
        yield( inputs[excerpt], targets[excerpt]) 
model_ckpt = 'model.ckpt'
new=True
if os.path.isfile(model_ckpt):
    input_var = None
    while input_var not in ['yes', 'no']:
        input_var = input("We found model.ckpt file. Do you want to load it [yes/no]?")
    if input_var == 'yes':
        new = False
result=[]
m=0
xx=0.0
# start training   siamese
with tf.Session() as sess:
#sess.run(tf.initialize_all_variables())  
    sess.run(tf.global_variables_initializer())
    if new:
        print('new')
        n_epoch = n_e
        batch_size =b_s
        for step in range(n_epoch):
            for  batch_x1, batch_y1 in minibatches(x_train, y_train, batch_size, shuffle=True):
                for  batch_x2, batch_y2 in minibatches(x1, y1, batch_size, shuffle=True):

                    batch_y = (batch_y1 == batch_y2).astype('float')
                    #print()
                    _, loss_v = sess.run([train_step, siames.loss ], feed_dict={
                            siames.x1: batch_x1,
                            siames.x2: batch_x2,
                            siames.y_: batch_y})
                    #print(loss_v)

                    if np.isnan(loss_v):
                        print('Model diverged with loss = NaN')
                        quit()
            
            if step % 10 == 0: 
                print ('step %d: loss %.3f' % (step, loss_v))

            if step % (n_e-1) == 0 and step > 0:
                m=0
            #saver.save(sess, r'F:\demo\python\my_code\cpkl\model.ckpt')
                for  batch_x, batch_y in minibatches(x_val, y_val, batch_size=1, shuffle=True):
                #print(batch_y,'y')
                    #print(batch_x,'batch')
                    batch_x1=batch_x.repeat(100,axis=0)
                    batch_y1=batch_y.repeat(100,axis=0)
                #print(batch_x1.shape,va.shape)
                    neg=siames.neg.eval( feed_dict={
                            siames.x1: x1,
                            siames.x2: batch_x1
                           })
#                for x in neg:
                    a=list(np.where(neg==np.min(neg,axis=0)))
                    #print(a,'sdsdd')
                    inde=[y1[x] for x in a[0]]
                    #print(l)
                    #for kk in a[0]:
                #a= neg.index(min(neg))  #
                     #   print(kk,l[kk],'sauhdfghsafdsfds')
                     #   inde.append()
                #inde=1
                    #print(inde,'inde',a[0])
                    #print(batch_y)
                    #print(batch_y,inde,'inde')
                    if batch_y in inde:
                        m+=1
                        ss[int(batch_y)-1]=ss[int(batch_y)-1]+1
                    result.append(batch_y)
                    result.append(inde)
                for x in range(100):
                    ss[x]=float(ss[x])/(nl-nu)
                rrr=len([x for x in ss if x>0.5])
                print(rrr)
                s.append(str(m/x_val.shape[0]))
                xx=max(xx,m/x_val.shape[0])
                print('loss')
                print(m/x_val.shape[0],x_val.shape,m,xx)
                #print()

        f.write(' '.join(rrr)+'    '+str(nu),'/n')
        f.close()
    else:
        pass
